{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKvFrWOSs85d"
      },
      "source": [
        "# NLP - Scalable, Accurate, Rule-Based pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y2giu9oqV0R"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üß© Mercury App Configuration\n",
        "# ============================================================\n",
        "import mercury as mr\n",
        "\n",
        "app = mr.App(\n",
        "    title=\"Advanced NLP Text Classification Dashboard - Multilingual\",\n",
        "    description=\"Upload CSV or Excel files to classify transcripts with intelligent language detection, translation, and sentiment analysis.\"\n",
        ")\n",
        "\n",
        "file = mr.File(label=\"üìÅ Upload Dataset (.csv or .xlsx)\")\n",
        "enable_translation = mr.Checkbox(label=\"üåç Enable Automatic Translation\", value=True)\n",
        "translation_confidence = mr.Slider(label=\"üéØ Translation Confidence Threshold\", value=0.7, min=0.0, max=1.0, step=0.05)\n",
        "run_button = mr.Button(label=\"üöÄ Run NLP Pipeline\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üì¶ Imports & Configuration\n",
        "# ============================================================\n",
        "import os, re, time, warnings, html, unicodedata\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from afinn import Afinn\n",
        "from langdetect import detect, detect_langs, DetectorFactory, LangDetectException\n",
        "from deep_translator import GoogleTranslator\n",
        "from typing import Dict, Tuple\n",
        "from collections import Counter\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "DetectorFactory.seed = 0\n",
        "af = Afinn()\n",
        "\n",
        "# Configuration\n",
        "NUM_THREADS = 8\n",
        "MIN_TEXT_LENGTH = 3\n",
        "MIN_TRANSLATION_LENGTH = 3\n",
        "SENTIMENT_WEIGHTS = {'textblob': 0.6, 'afinn': 0.4}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üìö Enhanced Category and Subcategory Keywords\n",
        "# ============================================================\n",
        "TOPIC_KEYWORDS = {\n",
        "    \"login issue\": [\n",
        "        \"login\", \"log in\", \"sign in\", \"sign-in\", \"signin\", \"sign out\", \"sign-out\", \"signout\",\n",
        "        \"password\", \"forgot password\", \"reset password\", \"authentication\",\n",
        "        \"verify account\", \"verification code\", \"2fa\", \"two-factor\", \"two factor\",\n",
        "        \"unable to access account\", \"can't log in\", \"cannot login\"\n",
        "    ],\n",
        "    \"account issue\": [\n",
        "        \"account\", \"profile\", \"username\", \"display name\",\n",
        "        \"linked account\", \"merge account\", \"multiple accounts\",\n",
        "        \"email change\", \"update details\", \"account disabled\",\n",
        "        \"account locked\", \"deactivate account\", \"delete account\"\n",
        "    ],\n",
        "    \"playback issue\": [\n",
        "        \"playback\", \"stream\", \"music not playing\", \"song not playing\",\n",
        "        \"track skipped\", \"buffering\", \"lag\", \"pause\", \"stuck\",\n",
        "        \"stops suddenly\", \"won't play\", \"audio issue\", \"no sound\",\n",
        "        \"silence\", \"volume problem\", \"audio quality\"\n",
        "    ],\n",
        "    \"device issue\": [\n",
        "        \"bluetooth\", \"speaker\", \"carplay\", \"android auto\", \"smart tv\",\n",
        "        \"echo\", \"alexa\", \"chromecast\", \"airplay\", \"headphones\",\n",
        "        \"device not showing\", \"device disconnected\", \"connection issue\"\n",
        "    ],\n",
        "    \"content restriction\": [\n",
        "        \"song not available\", \"track unavailable\", \"region restriction\",\n",
        "        \"country restriction\", \"not licensed\", \"greyed out\", \"removed song\",\n",
        "        \"can't find song\", \"missing track\"\n",
        "    ],\n",
        "    \"ad issue\": [\n",
        "        \"ads\", \"advertisement\", \"too many ads\", \"ad volume\",\n",
        "        \"ad playing\", \"premium ads\", \"commercials\", \"ad frequency\"\n",
        "    ],\n",
        "    \"recommendation issue\": [\n",
        "        \"recommendations\", \"discover weekly\", \"radio\", \"algorithm\",\n",
        "        \"curated\", \"autoplay\", \"song suggestions\", \"not relevant\",\n",
        "        \"bad recommendations\"\n",
        "    ],\n",
        "    \"ui issue\": [\n",
        "        \"interface\", \"layout\", \"design\", \"dark mode\", \"theme\",\n",
        "        \"buttons not working\", \"search not working\", \"filter not working\",\n",
        "        \"navigation\", \"menu\"\n",
        "    ],\n",
        "    \"general feedback\": [\n",
        "        \"suggestion\", \"feedback\", \"recommend\", \"love spotify\",\n",
        "        \"like app\", \"app improvement\", \"feature request\", \"enhancement\"\n",
        "    ],\n",
        "    \"network failure\": [\n",
        "        \"network\", \"connectivity\", \"internet\", \"server\",\n",
        "        \"connection failed\", \"offline\", \"not connecting\",\n",
        "        \"spotify down\", \"timeout\", \"dns\", \"proxy\", \"vpn\"\n",
        "    ],\n",
        "    \"app crash\": [\n",
        "        \"crash\", \"crashed\", \"app closed\", \"stopped working\", \"freeze\",\n",
        "        \"freezing\", \"hang\", \"bug\", \"error message\", \"glitch\",\n",
        "        \"unresponsive\", \"not responding\"\n",
        "    ],\n",
        "    \"performance issue\": [\n",
        "        \"slow\", \"lag\", \"delay\", \"performance\", \"loading\", \"slow loading\",\n",
        "        \"takes forever\", \"laggy\"\n",
        "    ],\n",
        "    \"data sync issue\": [\n",
        "        \"sync\", \"not syncing\", \"listening history\", \"recently played\",\n",
        "        \"activity feed\", \"spotify connect\", \"data lost\", \"missing data\",\n",
        "        \"playlist not syncing\"\n",
        "    ],\n",
        "    \"subscription issue\": [\n",
        "        \"subscription\", \"plan\", \"premium\", \"cancel\", \"renew\",\n",
        "        \"billing\", \"charged\", \"payment\", \"refund\", \"invoice\",\n",
        "        \"upgrade\", \"downgrade\", \"free trial\", \"family plan\",\n",
        "        \"student plan\", \"gift card\", \"promo code\", \"spotify wrapped\",\n",
        "        \"card\", \"payment failed\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "SUBCATEGORY_KEYWORDS = {\n",
        "    \"subscription issue\": {\n",
        "        \"payment\": [\"refund\", \"charged\", \"billing\", \"invoice\", \"payment\", \"payment failed\", \"card declined\"],\n",
        "        \"cancel\": [\"cancel\", \"unsubscribe\", \"stop subscription\", \"end subscription\"],\n",
        "        \"upgrade\": [\"upgrade\", \"family plan\", \"student plan\", \"premium\", \"switch plan\"],\n",
        "    },\n",
        "    \"account issue\": {\n",
        "        \"login\": [\"login\", \"password\", \"signin\", \"sign in\", \"authentication\"],\n",
        "        \"profile\": [\"profile\", \"email\", \"username\", \"display name\", \"account settings\"],\n",
        "    },\n",
        "    \"device issue\": {\n",
        "        \"mobile\": [\"phone\", \"android\", \"iphone\", \"ios\", \"mobile app\"],\n",
        "        \"car\": [\"carplay\", \"android auto\", \"car\", \"vehicle\"],\n",
        "        \"smart_device\": [\"alexa\", \"echo\", \"chromecast\", \"smart tv\", \"airplay\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Enhanced sentiment keywords\n",
        "SENTIMENT_KEYWORDS = {\n",
        "    \"very_negative\": [\n",
        "        \"bad\", \"poor\", \"terrible\", \"awful\", \"worst\", \"horrible\", \"pathetic\",\n",
        "        \"disappointing\", \"useless\", \"waste\", \"boring\", \"dull\", \"confusing\",\n",
        "        \"frustrating\", \"annoying\", \"unpleasant\", \"uncomfortable\"\n",
        "    ],\n",
        "    \"moderate_negative\": [\n",
        "        \"cold\", \"asleep\", \"feeling asleep\", \"sleepy\", \"tired\", \"exhausting\",\n",
        "        \"no equipment\", \"no tools\", \"lack of\", \"missing\", \"insufficient\"\n",
        "    ],\n",
        "    \"negative_phrases\": [\n",
        "        \"room was cold\", \"cold room\", \"feeling asleep\",\n",
        "        \"no equipment\", \"had to write everything\", \"lack of equipment\"\n",
        "    ],\n",
        "    \"positive\": [\n",
        "        \"very good\", \"excellent\", \"engaging\", \"superb\", \"amazing\",\n",
        "        \"all good\", \"good going\", \"overall nice\", \"good\", \"interactive\",\n",
        "        \"mind blowing\", \"service\", \"experience\", \"overall good\",\n",
        "        \"everything good\", \"rocking\", \"everything is fine\",\n",
        "        \"informative\", \"helpful\", \"great\", \"wonderful\", \"fantastic\", \"awesome\", \"outstanding\",\n",
        "        \"brilliant\", \"impressive\", \"valuable\", \"useful\", \"beneficial\", \"effective\",\n",
        "        \"satisfactory\", \"satisfied\", \"pleased\", \"happy\", \"enjoy\", \"enjoyed\", \"loved\", \"perfect\"\n",
        "    ],\n",
        "    \"neutral_phrases\": [\n",
        "        \"nothing else\", \"no additional comments\", \"no comments\", \"nothing\",\n",
        "        \"nothing to add\", \"no comment\", \"none so far\", \"no other comment\", \"nothing more\",\n",
        "        \"no more comments\", \"nothing in specific\", \"nothing specific\", \"nothing in particular\"\n",
        "    ],\n",
        "    \"meaningless_patterns\": [\n",
        "        r\"^[a-zA-Z]$\", r\"^[0-9]+$\",\n",
        "        r\"^(na|n/a|n\\.a|n\\|a|n\\\\a|n\\?a|ma|n\\./a|n-a)$\",\n",
        "        r\"^(nil|none|non|nope)$\",\n",
        "        r\"^(ok|okay|yes|no|y|n)$\",\n",
        "        r\"^[^\\w\\s]+$\",\n",
        "        r\"^(.)\\1+$\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Pre-compile regex patterns\n",
        "CONSUMER_PATTERN_PRIMARY = re.compile(\n",
        "    r\"(?i)Consumer:\\s*(.*?)(?=\\s*\\|\\s*\\d{4}-\\d{2}-\\d{2}|$|\\s*\\|\\s*Agent:)\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "CONSUMER_PATTERN_FALLBACK = re.compile(\n",
        "    r\"(?i)Consumer:\\s*(.*?)(?=\\||$)\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "# Text cleaning regex patterns\n",
        "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "EMAIL_RE = re.compile(r\"\\S+@\\S+\\.\\S+\")\n",
        "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "MULTI_WS_RE = re.compile(r\"\\s+\")\n",
        "REPEATED_PUNCT_RE = re.compile(r\"([!?.,])\\1{1,}\")\n",
        "NON_PRINTABLE_RE = re.compile(r\"[\\x00-\\x1f\\x7f-\\x9f]\")\n",
        "EMOJI_RE = re.compile(\n",
        "    \"[\"\n",
        "    \"\\U0001F300-\\U0001F6FF\"\n",
        "    \"\\U0001F900-\\U0001F9FF\"\n",
        "    \"\\U0001F1E0-\\U0001F1FF\"\n",
        "    \"\\U00002700-\\U000027BF\"\n",
        "    \"]+\", flags=re.UNICODE\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üßπ Enhanced Text Cleaning\n",
        "# ============================================================\n",
        "\n",
        "def clean_text(raw):\n",
        "    \"\"\"Enhanced text cleaning with better normalization\"\"\"\n",
        "    if raw is None or pd.isna(raw):\n",
        "        return \"\"\n",
        "\n",
        "    s = str(raw).strip()\n",
        "\n",
        "    # Handle common encoding issues\n",
        "    s = s.replace('√É¬±', '√±').replace('√É¬°', '√°').replace('√É¬©', '√©')\n",
        "    s = s.replace('√É¬≠', '√≠').replace('√É¬≥', '√≥').replace('√É¬∫', '√∫')\n",
        "\n",
        "    s = html.unescape(s)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "\n",
        "    s = URL_RE.sub(\" \", s)\n",
        "    s = EMAIL_RE.sub(\" \", s)\n",
        "    s = HTML_TAG_RE.sub(\" \", s)\n",
        "    s = EMOJI_RE.sub(\" \", s)\n",
        "    s = NON_PRINTABLE_RE.sub(\" \", s)\n",
        "    s = REPEATED_PUNCT_RE.sub(lambda m: m.group(1), s)\n",
        "\n",
        "    s = s.strip()\n",
        "    s = MULTI_WS_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"^[^\\w']+|[^\\w']+$\", \"\", s)\n",
        "\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üîç Intelligent Meaningless Text Detection\n",
        "# ============================================================\n",
        "\n",
        "def is_meaningless_text(text):\n",
        "    \"\"\"\n",
        "    Advanced detection of meaningless text using multiple heuristics.\n",
        "    Returns (is_meaningless: bool, reason: str)\n",
        "    \"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return True, \"empty\"\n",
        "\n",
        "    text = text.strip()\n",
        "    t_lower = text.lower()\n",
        "\n",
        "    # Normalize variations for comparison\n",
        "    t_normalized = t_lower.replace(\"/\", \"\").replace(\"\\\\\", \"\").replace(\"|\", \"\")\n",
        "    t_normalized = t_normalized.replace(\".\", \"\").replace(\"-\", \"\").replace(\"?\", \"\")\n",
        "    t_normalized = t_normalized.replace(\" \", \"\").replace(\"_\", \"\")\n",
        "\n",
        "    # Check normalized against meaningless patterns\n",
        "    meaningless_normalized = [\"na\", \"ma\", \"nil\", \"none\", \"non\", \"ok\", \"yes\", \"no\"]\n",
        "    if t_normalized in meaningless_normalized:\n",
        "        return True, f\"meaningless_normalized: {t_normalized}\"\n",
        "\n",
        "    # Check against meaningless patterns\n",
        "    for pattern in SENTIMENT_KEYWORDS[\"meaningless_patterns\"]:\n",
        "        if re.match(pattern, t_lower, re.IGNORECASE):\n",
        "            return True, f\"matched_pattern: {pattern}\"\n",
        "\n",
        "    # Single character (except 'i' or 'a' which could be meaningful)\n",
        "    if len(text) == 1 and text.lower() not in ['i', 'a']:\n",
        "        return True, \"single_character\"\n",
        "\n",
        "    # Only punctuation or special characters\n",
        "    if not any(c.isalnum() for c in text):\n",
        "        return True, \"no_alphanumeric\"\n",
        "\n",
        "    # Very short and no meaningful words\n",
        "    if len(text) <= 4:\n",
        "        meaningless_short = ['na', 'n/a', 'na.', 'nil', 'ok', 'no', 'yes', 'g', 'ma']\n",
        "        if t_lower in meaningless_short or t_normalized in meaningless_short:\n",
        "            return True, \"short_meaningless\"\n",
        "\n",
        "    # Check word count\n",
        "    words = re.findall(r\"\\b[a-zA-Z]+\\b\", text)\n",
        "    if len(words) == 0:\n",
        "        return True, \"no_words\"\n",
        "\n",
        "    # Repeated same word\n",
        "    if len(words) > 1 and len(set(words)) == 1:\n",
        "        return True, \"repeated_word\"\n",
        "\n",
        "    # High ratio of numbers to text\n",
        "    num_count = sum(c.isdigit() for c in text)\n",
        "    if len(text) > 0 and num_count / len(text) > 0.5:\n",
        "        return True, \"mostly_numbers\"\n",
        "\n",
        "    return False, \"meaningful\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üåç Advanced Language Detection\n",
        "# ============================================================\n",
        "\n",
        "def detect_language_with_confidence(text):\n",
        "    \"\"\"\n",
        "    Enhanced language detection with confidence scoring.\n",
        "    Returns (language_code, confidence_score)\n",
        "    \"\"\"\n",
        "    if not text or len(text) < 3:\n",
        "        return \"en\", 1.0\n",
        "\n",
        "    try:\n",
        "        # Spanish indicators check FIRST\n",
        "        spanish_indicators = [\n",
        "            'tengo', 'nada', 'mas', 'que', 'agregar', 'muy', 'esta', 'dia',\n",
        "            'gracias', 'como', 'mejorar', 'estoy', 'trabajo', 'comentarios',\n",
        "            'adicionales', 'cada', 'aprendo', 'agradezco', 'tiempo', 'dedicacion',\n",
        "            'ningun', 'fue', 'excelente', 'sesion'\n",
        "        ]\n",
        "\n",
        "        spanish_chars = ['√°', '√©', '√≠', '√≥', '√∫', '√±', '√º', '√É', '√É¬±', '√É¬°', '√É¬©', '√É¬≠', '√É¬≥', '√É¬∫']\n",
        "        has_spanish_chars = any(char in text for char in spanish_chars)\n",
        "\n",
        "        text_lower = text.lower()\n",
        "        words_in_text = set(re.findall(r'\\b\\w+\\b', text_lower))\n",
        "\n",
        "        spanish_word_matches = len(words_in_text.intersection(spanish_indicators))\n",
        "\n",
        "        if has_spanish_chars or spanish_word_matches >= 2:\n",
        "            return \"es\", 0.95\n",
        "\n",
        "        # Check other Romance languages\n",
        "        romance_indicators = {\n",
        "            'fr': ['je', 'tu', 'est', 'avec', 'pour', 'dans', 'mais', 'bien', 'tr√®s', 'merci'],\n",
        "            'pt': ['eu', 'voc√™', 'est√°', 'para', 'com', 'mas', 'bem', 'muito', 'obrigado'],\n",
        "            'it': ['io', 'tu', 'con', 'per', 'ma', 'bene', 'molto', 'grazie']\n",
        "        }\n",
        "\n",
        "        for lang_code, indicators in romance_indicators.items():\n",
        "            lang_matches = len(words_in_text.intersection(indicators))\n",
        "            if lang_matches >= 2:\n",
        "                return lang_code, 0.90\n",
        "\n",
        "        # English indicators\n",
        "        common_english_words = [\n",
        "            'the', 'is', 'are', 'was', 'were', 'have', 'has', 'had', 'will', 'would',\n",
        "            'could', 'should', 'can', 'may', 'must', 'do', 'does', 'did',\n",
        "            'a', 'an', 'and', 'or', 'but', 'if', 'then', 'than',\n",
        "            'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'\n",
        "        ]\n",
        "\n",
        "        english_word_matches = len(words_in_text.intersection(common_english_words))\n",
        "\n",
        "        if english_word_matches >= 3:\n",
        "            return \"en\", 0.99\n",
        "\n",
        "        if english_word_matches >= 2 and len(text.split()) <= 15:\n",
        "            return \"en\", 0.95\n",
        "\n",
        "        # ASCII ratio check\n",
        "        ascii_ratio = sum(ord(c) < 128 for c in text) / len(text)\n",
        "        if ascii_ratio > 0.95 and english_word_matches >= 1 and spanish_word_matches == 0:\n",
        "            return \"en\", 0.90\n",
        "\n",
        "        # Get language probabilities from langdetect\n",
        "        lang_probs = detect_langs(text)\n",
        "        top_lang = lang_probs[0]\n",
        "        lang_code = top_lang.lang\n",
        "        confidence = top_lang.prob\n",
        "\n",
        "        # Override if strong English indicators\n",
        "        if lang_code not in ['en', 'es', 'fr', 'pt', 'it'] and english_word_matches >= 2:\n",
        "            return \"en\", 0.85\n",
        "\n",
        "        return lang_code, confidence\n",
        "\n",
        "    except LangDetectException:\n",
        "        if text:\n",
        "            ascii_ratio = sum(ord(c) < 128 for c in text) / len(text)\n",
        "            if ascii_ratio > 0.9:\n",
        "                return \"en\", 0.6\n",
        "        return \"unknown\", 0.0\n",
        "    except Exception as e:\n",
        "        return \"unknown\", 0.0\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üîÑ Intelligent Translation\n",
        "# ============================================================\n",
        "\n",
        "def smart_translate(text, lang_code, confidence, threshold):\n",
        "    \"\"\"\n",
        "    Intelligent translation with confidence-based decisions.\n",
        "    Returns (translated_text, was_translated, translation_info)\n",
        "    \"\"\"\n",
        "    if lang_code == 'en':\n",
        "        return text, False, \"already_english\"\n",
        "\n",
        "    # Romance languages use lower threshold\n",
        "    if lang_code in ['es', 'fr', 'pt', 'it']:\n",
        "        if confidence < 0.5:\n",
        "            return text, False, f\"low_confidence_{confidence:.2f}\"\n",
        "    else:\n",
        "        if confidence < threshold:\n",
        "            return text, False, f\"low_confidence_{confidence:.2f}\"\n",
        "\n",
        "    if len(text) < MIN_TRANSLATION_LENGTH:\n",
        "        short_translatable = ['gracias', 'merci', 'obrigado', 'grazie', 'si', 'oui', 'sim']\n",
        "        if text.lower() not in short_translatable:\n",
        "            return text, False, \"too_short\"\n",
        "\n",
        "    # Check for English words (skip for Romance languages)\n",
        "    if lang_code not in ['es', 'fr', 'pt', 'it']:\n",
        "        common_english_words = [\n",
        "            'the', 'is', 'are', 'was', 'were', 'have', 'has', 'had', 'will',\n",
        "            'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
        "            'not', 'more', 'all', 'some', 'any', 'learned', 'implementing'\n",
        "        ]\n",
        "\n",
        "        text_lower = text.lower()\n",
        "        words_in_text = set(re.findall(r'\\b\\w+\\b', text_lower))\n",
        "        english_word_matches = len(words_in_text.intersection(common_english_words))\n",
        "\n",
        "        if english_word_matches >= 2:\n",
        "            return text, False, f\"likely_english_{english_word_matches}_matches\"\n",
        "\n",
        "    # Attempt translation\n",
        "    try:\n",
        "        translator = GoogleTranslator(source='auto', target='en')\n",
        "        translated = translator.translate(text)\n",
        "\n",
        "        # Check similarity (skip for Romance languages)\n",
        "        if lang_code not in ['es', 'fr', 'pt', 'it']:\n",
        "            similarity_ratio = sum(a == b for a, b in zip(text.lower(), translated.lower())) / max(len(text), len(translated))\n",
        "            if similarity_ratio > 0.8:\n",
        "                return text, False, f\"too_similar_{similarity_ratio:.2f}\"\n",
        "\n",
        "        return translated, True, f\"success_{lang_code}\"\n",
        "    except Exception as e:\n",
        "        return text, False, f\"failed_{str(e)[:30]}\"\n",
        "\n",
        "\n",
        "def extract_consumer_text(transcript: str) -> str:\n",
        "    \"\"\"Extract consumer text from transcript.\"\"\"\n",
        "    if not isinstance(transcript, str):\n",
        "        return \"\"\n",
        "\n",
        "    parts = CONSUMER_PATTERN_PRIMARY.findall(transcript + \" \")\n",
        "    if not parts:\n",
        "        parts = CONSUMER_PATTERN_FALLBACK.findall(transcript + \"|\")\n",
        "\n",
        "    return \" \".join(p.strip() for p in parts if p.strip())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üí≠ Enhanced Sentiment Classification\n",
        "# ============================================================\n",
        "\n",
        "def has_positive_override(text):\n",
        "    \"\"\"Detect positive sentiment despite negative-sounding start\"\"\"\n",
        "    if not text:\n",
        "        return False\n",
        "\n",
        "    t = text.lower().strip()\n",
        "\n",
        "    positive_override_patterns = [\n",
        "        r\"nothing.*(?:happy|good|great|excellent|awesome|perfect|love|amazing)\",\n",
        "        r\"no.*(?:complain|issue|problem).*(?:good|great|excellent|awesome|perfect)\",\n",
        "    ]\n",
        "\n",
        "    for pattern in positive_override_patterns:\n",
        "        if re.search(pattern, t):\n",
        "            return True\n",
        "\n",
        "    if t.startswith((\"nothing\", \"no other\", \"no \")):\n",
        "        positive_indicators = [\n",
        "            \"happy\", \"good\", \"great\", \"excellent\", \"awesome\", \"perfect\",\n",
        "            \"amazing\", \"wonderful\", \"fantastic\", \"love\", \"enjoyed\", \"helpful\"\n",
        "        ]\n",
        "        if any(indicator in t for indicator in positive_indicators):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def classify_sentiment_with_confidence(text):\n",
        "    \"\"\"\n",
        "    Enhanced sentiment classification with confidence scoring.\n",
        "    Returns (sentiment, confidence_score)\n",
        "    \"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return \"\", 0.0\n",
        "\n",
        "    t_lower = text.lower().strip()\n",
        "\n",
        "    if is_meaningless_text(text)[0]:\n",
        "        return \"\", 1.0\n",
        "\n",
        "    # Check neutral phrases first\n",
        "    for phrase in SENTIMENT_KEYWORDS.get(\"neutral_phrases\", []):\n",
        "        if phrase in t_lower:\n",
        "            strong_positive = [\"excellent\", \"amazing\", \"wonderful\", \"fantastic\", \"awesome\", \"loved\", \"perfect\"]\n",
        "            strong_negative = [\"terrible\", \"awful\", \"horrible\", \"worst\", \"pathetic\", \"useless\", \"waste\"]\n",
        "\n",
        "            has_strong_positive = any(word in t_lower for word in strong_positive)\n",
        "            has_strong_negative = any(word in t_lower for word in strong_negative)\n",
        "\n",
        "            if not has_strong_positive and not has_strong_negative:\n",
        "                return \"neutral\", 0.85\n",
        "\n",
        "    # Negative detection\n",
        "    for phrase in SENTIMENT_KEYWORDS.get(\"negative_phrases\", []):\n",
        "        if phrase.lower() in t_lower:\n",
        "            return \"negative\", 0.9\n",
        "\n",
        "    for keyword in SENTIMENT_KEYWORDS[\"very_negative\"]:\n",
        "        if keyword.lower() in t_lower:\n",
        "            return \"negative\", 0.85\n",
        "\n",
        "    for keyword in SENTIMENT_KEYWORDS.get(\"moderate_negative\", []):\n",
        "        if keyword.lower() in t_lower:\n",
        "            return \"negative\", 0.75\n",
        "\n",
        "    # Positive detection\n",
        "    if has_positive_override(text):\n",
        "        return \"positive\", 0.9\n",
        "\n",
        "    for keyword in SENTIMENT_KEYWORDS[\"positive\"]:\n",
        "        if keyword.lower() in t_lower:\n",
        "            return \"positive\", 0.85\n",
        "\n",
        "    # Hybrid scoring\n",
        "    try:\n",
        "        tb_score = TextBlob(text).sentiment.polarity\n",
        "        af_score = af.score(text) / 5.0\n",
        "        combined_score = 0.6 * tb_score + 0.4 * af_score\n",
        "\n",
        "        confidence = min(abs(combined_score) * 2, 0.9)\n",
        "\n",
        "        if combined_score <= -0.15:\n",
        "            return \"negative\", confidence\n",
        "        elif combined_score >= 0.15:\n",
        "            return \"positive\", confidence\n",
        "        else:\n",
        "            return \"neutral\", max(confidence, 0.6)\n",
        "    except:\n",
        "        return \"neutral\", 0.3\n",
        "\n",
        "\n",
        "def predict_category(text: str) -> Tuple[str, int]:\n",
        "    \"\"\"Predict category with confidence score.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    best_match, best_score = \"\", 0\n",
        "\n",
        "    for category, keywords in TOPIC_KEYWORDS.items():\n",
        "        score = 0\n",
        "        for keyword in keywords:\n",
        "            if len(keyword.split()) == 1 and len(keyword) <= 3:\n",
        "                pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
        "                if re.search(pattern, text_lower):\n",
        "                    score += 1\n",
        "            else:\n",
        "                if keyword in text_lower:\n",
        "                    score += 1.5 if ' ' in keyword else 1\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_match = category\n",
        "\n",
        "    return (best_match if best_score > 0 else \"\", int(best_score))\n",
        "\n",
        "\n",
        "def predict_subcategory(category: str, text: str) -> Tuple[str, int]:\n",
        "    \"\"\"Predict subcategory based on category.\"\"\"\n",
        "    if not category or category not in SUBCATEGORY_KEYWORDS:\n",
        "        return (\"\", 0)\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    best_match, best_score = \"\", 0\n",
        "\n",
        "    for subcategory, keywords in SUBCATEGORY_KEYWORDS[category].items():\n",
        "        score = 0\n",
        "        for keyword in keywords:\n",
        "            if len(keyword.split()) == 1 and len(keyword) <= 3:\n",
        "                pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
        "                if re.search(pattern, text_lower):\n",
        "                    score += 1\n",
        "            else:\n",
        "                if keyword in text_lower:\n",
        "                    score += 1.5 if ' ' in keyword else 1\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_match = subcategory\n",
        "\n",
        "    return (best_match, int(best_score))\n",
        "\n",
        "\n",
        "def apply_rules(text: str, preds: Dict) -> Dict:\n",
        "    \"\"\"Apply rule-based overrides.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    if any(k in text_lower for k in [\"refund\", \"charged\", \"billing\", \"payment failed\"]):\n",
        "        preds[\"category\"] = \"subscription issue\"\n",
        "        preds[\"subcategory\"] = \"payment\"\n",
        "        if \"refund\" in text_lower or \"charged\" in text_lower:\n",
        "            preds[\"sentiment\"] = \"negative\"\n",
        "    elif \"cancel\" in text_lower and \"subscription\" in text_lower:\n",
        "        preds[\"category\"] = \"subscription issue\"\n",
        "        preds[\"subcategory\"] = \"cancel\"\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üîÑ Core Row Processing\n",
        "# ============================================================\n",
        "\n",
        "def process_row(row: Dict, translation_enabled: bool, trans_threshold: float) -> Dict:\n",
        "    \"\"\"Process a single transcript row with comprehensive analysis.\"\"\"\n",
        "    conversation_id = row.get(\"Conversation Id\", \"\")\n",
        "    transcript = str(row.get(\"transcripts\", \"\"))\n",
        "    consumer_text = extract_consumer_text(transcript)\n",
        "\n",
        "    # Clean text\n",
        "    cleaned_text = clean_text(consumer_text)\n",
        "\n",
        "    # Handle empty text\n",
        "    if not cleaned_text.strip():\n",
        "        return {\n",
        "            \"Conversation Id\": conversation_id,\n",
        "            \"Consumer_Text\": consumer_text,\n",
        "            \"Cleaned_Text\": cleaned_text,\n",
        "            \"Translated_Text\": \"\",\n",
        "            \"Category\": \"\",\n",
        "            \"Subcategory\": \"\",\n",
        "            \"Sentiment\": \"\",\n",
        "        }\n",
        "\n",
        "    # Check if meaningless\n",
        "    is_meaningless, reason = is_meaningless_text(cleaned_text)\n",
        "    if is_meaningless:\n",
        "        return {\n",
        "            \"Conversation Id\": conversation_id,\n",
        "            \"Consumer_Text\": consumer_text,\n",
        "            \"Cleaned_Text\": cleaned_text,\n",
        "            \"Translated_Text\": \"\",\n",
        "            \"Category\": \"\",\n",
        "            \"Subcategory\": \"\",\n",
        "            \"Sentiment\": \"\",\n",
        "        }\n",
        "\n",
        "    # Detect language\n",
        "    lang_code, lang_confidence = detect_language_with_confidence(cleaned_text)\n",
        "\n",
        "    # Initialize translation fields\n",
        "    translated_text = \"\"\n",
        "    text_for_analysis = cleaned_text\n",
        "\n",
        "    # Translate if needed\n",
        "    if translation_enabled and lang_code != 'en':\n",
        "        translated, was_translated, trans_info = smart_translate(cleaned_text, lang_code, lang_confidence, trans_threshold)\n",
        "        if was_translated:\n",
        "            translated_text = translated\n",
        "            text_for_analysis = translated\n",
        "\n",
        "    # Handle unknown or very short text\n",
        "    if lang_code == 'unknown' or len(text_for_analysis.split()) < MIN_TEXT_LENGTH:\n",
        "        return {\n",
        "            \"Conversation Id\": conversation_id,\n",
        "            \"Consumer_Text\": consumer_text,\n",
        "            \"Cleaned_Text\": cleaned_text,\n",
        "            \"Translated_Text\": translated_text,\n",
        "            \"Category\": \"\",\n",
        "            \"Subcategory\": \"\",\n",
        "            \"Sentiment\": \"\",\n",
        "        }\n",
        "\n",
        "    # Predict category and subcategory\n",
        "    category, cat_confidence = predict_category(text_for_analysis)\n",
        "    subcategory, subcat_confidence = predict_subcategory(category, text_for_analysis)\n",
        "    sentiment, sent_confidence = classify_sentiment_with_confidence(text_for_analysis)\n",
        "\n",
        "    # Build predictions\n",
        "    preds = {\n",
        "        \"category\": category,\n",
        "        \"subcategory\": subcategory,\n",
        "        \"sentiment\": sentiment,\n",
        "    }\n",
        "\n",
        "    # Apply rules\n",
        "    preds = apply_rules(text_for_analysis, preds)\n",
        "\n",
        "    return {\n",
        "        \"Conversation Id\": conversation_id,\n",
        "        \"Consumer_Text\": consumer_text,\n",
        "        \"Cleaned_Text\": cleaned_text,\n",
        "        \"Translated_Text\": translated_text,\n",
        "        \"Category\": preds[\"category\"],\n",
        "        \"Subcategory\": preds[\"subcategory\"],\n",
        "        \"Sentiment\": preds[\"sentiment\"],\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üßÆ Run NLP Pipeline\n",
        "# ============================================================\n",
        "\n",
        "def run_pipeline(uploaded_file, translation_enabled, trans_threshold):\n",
        "    start = time.time()\n",
        "\n",
        "    file_path = uploaded_file.filepath\n",
        "    original_name = getattr(uploaded_file, 'filename', '') or getattr(uploaded_file, 'name', '')\n",
        "\n",
        "    # Load dataset\n",
        "    df = None\n",
        "    error_msg = \"\"\n",
        "\n",
        "    if original_name.lower().endswith('.xlsx'):\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Excel read error: {str(e)}\"\n",
        "    elif original_name.lower().endswith('.csv'):\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "        except Exception as e:\n",
        "            error_msg = f\"CSV read error: {str(e)}\"\n",
        "\n",
        "    if df is None:\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "        except:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Could not read file. {error_msg}. Last attempt: {str(e)}\")\n",
        "\n",
        "    if df is None:\n",
        "        raise ValueError(\"Unsupported file format. Upload a .csv or .xlsx file.\")\n",
        "\n",
        "    if \"Conversation Id\" not in df.columns or \"transcripts\" not in df.columns:\n",
        "        raise ValueError(\"Input file must contain 'Conversation Id' and 'transcripts' columns.\")\n",
        "\n",
        "    rows = df.to_dict(\"records\")\n",
        "    total_rows = len(rows)\n",
        "\n",
        "    results = []\n",
        "    processed = 0\n",
        "\n",
        "    # Parallel processing\n",
        "    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
        "        future_to_row = {executor.submit(process_row, row, translation_enabled, trans_threshold): row for row in rows}\n",
        "\n",
        "        for future in as_completed(future_to_row):\n",
        "            results.append(future.result())\n",
        "            processed += 1\n",
        "\n",
        "            if processed % max(1, total_rows // 10) == 0:\n",
        "                progress_pct = (processed / total_rows) * 100\n",
        "                print(f\"‚è≥ Progress: {processed:,}/{total_rows:,} ({progress_pct:.1f}%)\")\n",
        "\n",
        "    # Create output dataframe\n",
        "    out_df = pd.DataFrame(results)\n",
        "\n",
        "    # Save to file\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_filename = f\"sentiment_output_{timestamp}.csv\"\n",
        "    output_path = os.path.abspath(output_filename)\n",
        "    out_df.to_csv(output_path, index=False)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"‚úÖ Completed in {elapsed:.2f}s. Processed {len(out_df)} rows.\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    stats = {\n",
        "        'total_rows': total_rows,\n",
        "        'valid_categories': (out_df['Category'] != '').sum(),\n",
        "        'valid_subcategories': (out_df['Subcategory'] != '').sum(),\n",
        "        'valid_sentiments': (out_df['Sentiment'] != '').sum(),\n",
        "        'translated_count': (out_df['Translated_Text'] != '').sum(),\n",
        "        'meaningless_count': ((out_df['Category'] == '') & (out_df['Sentiment'] == '')).sum(),\n",
        "        'elapsed_time': elapsed,\n",
        "    }\n",
        "\n",
        "    return out_df, output_filename, output_path, stats\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üñ•Ô∏è Execute & Display Results\n",
        "# ============================================================\n",
        "if run_button.clicked:\n",
        "    if file is None or not hasattr(file, 'filepath') or file.filepath is None:\n",
        "        mr.Markdown(\"### ‚ö†Ô∏è Please upload a .csv or .xlsx file before running the pipeline.\")\n",
        "    else:\n",
        "        try:\n",
        "            mr.Markdown(\"### üîÑ Processing...\")\n",
        "            mr.Markdown(f\"**Translation enabled:** {'Yes ‚úì' if enable_translation.value else 'No ‚úó'}\")\n",
        "            mr.Markdown(f\"**Translation confidence threshold:** {translation_confidence.value:.2f}\")\n",
        "\n",
        "            df_result, output_filename, output_path, stats = run_pipeline(\n",
        "                file,\n",
        "                enable_translation.value,\n",
        "                translation_confidence.value\n",
        "            )\n",
        "\n",
        "            mr.Markdown(\"### ‚úÖ Processing Complete!\")\n",
        "            mr.Markdown(f\"**Total rows processed:** {stats['total_rows']:,}\")\n",
        "            mr.Markdown(f\"**Processing time:** {stats['elapsed_time']:.2f} seconds\")\n",
        "            mr.Markdown(f\"**Processing speed:** {stats['total_rows']/stats['elapsed_time']:.1f} rows/sec\")\n",
        "\n",
        "            # Statistics\n",
        "            mr.Markdown(\"---\")\n",
        "            mr.Markdown(\"### üìä Classification Statistics\")\n",
        "            mr.Markdown(f\"**Valid Category Classifications:** {stats['valid_categories']:,} ({stats['valid_categories']/stats['total_rows']*100:.1f}%)\")\n",
        "            mr.Markdown(f\"**Valid Subcategory Classifications:** {stats['valid_subcategories']:,} ({stats['valid_subcategories']/stats['total_rows']*100:.1f}%)\")\n",
        "            mr.Markdown(f\"**Valid Sentiment Classifications:** {stats['valid_sentiments']:,} ({stats['valid_sentiments']/stats['total_rows']*100:.1f}%)\")\n",
        "            mr.Markdown(f\"**Meaningless/Filtered Texts:** {stats['meaningless_count']:,} ({stats['meaningless_count']/stats['total_rows']*100:.1f}%)\")\n",
        "\n",
        "            if enable_translation.value:\n",
        "                mr.Markdown(f\"**Translated Texts:** {stats['translated_count']:,} ({stats['translated_count']/stats['total_rows']*100:.1f}%)\")\n",
        "\n",
        "            # Category distribution\n",
        "            mr.Markdown(\"---\")\n",
        "            mr.Markdown(\"### üìÇ Top Categories\")\n",
        "            category_counts = df_result[df_result['Category'] != '']['Category'].value_counts().head(5)\n",
        "            for category, count in category_counts.items():\n",
        "                mr.Markdown(f\"- **{category}**: {count:,} ({count/stats['total_rows']*100:.1f}%)\")\n",
        "\n",
        "            # Sentiment distribution\n",
        "            mr.Markdown(\"---\")\n",
        "            mr.Markdown(\"### üí≠ Sentiment Distribution\")\n",
        "            sentiment_counts = df_result['Sentiment'].value_counts()\n",
        "            for sentiment in ['positive', 'neutral', 'negative', '']:\n",
        "                count = sentiment_counts.get(sentiment, 0)\n",
        "                label = sentiment.upper() if sentiment else \"BLANK (meaningless)\"\n",
        "                mr.Markdown(f\"- **{label}**: {count:,} ({count/stats['total_rows']*100:.1f}%)\")\n",
        "\n",
        "            # Sample comments\n",
        "            mr.Markdown(\"---\")\n",
        "            mr.Markdown(\"### üìù Sample Comments by Sentiment\")\n",
        "            for sentiment in ['positive', 'neutral', 'negative']:\n",
        "                sample = df_result[df_result['Sentiment'] == sentiment].head(2)\n",
        "                if not sample.empty:\n",
        "                    mr.Markdown(f\"**{sentiment.upper()}:**\")\n",
        "                    for _, row in sample.iterrows():\n",
        "                        text = row['Translated_Text'] if row['Translated_Text'] else row['Cleaned_Text']\n",
        "                        trans_marker = \" [Translated]\" if row['Translated_Text'] else \"\"\n",
        "                        mr.Markdown(f\"- {text[:100]}...{trans_marker}\")\n",
        "\n",
        "            # Display results preview\n",
        "            mr.Markdown(\"---\")\n",
        "            mr.Markdown(\"### üìä Results Preview (First 50 rows)\")\n",
        "            df_result.head(50)\n",
        "\n",
        "            # Download instructions\n",
        "            mr.Markdown(\"---\")\n",
        "            mr.Markdown(\"### üì• Your Results are Ready!\")\n",
        "            mr.Markdown(f\"**‚úÖ File successfully created:** `{output_filename}`\")\n",
        "            mr.Markdown(\"\")\n",
        "            mr.Markdown(\"### üîΩ How to Download:\")\n",
        "            mr.Markdown(\"**File Location:**\")\n",
        "            mr.Markdown(f\"```\\n{output_path}\\n```\")\n",
        "            mr.Markdown(\"\")\n",
        "            mr.Markdown(\"**Download Options:**\")\n",
        "            mr.Markdown(\"1. Navigate to the file location on your server\")\n",
        "            mr.Markdown(\"2. Use your working directory if running Mercury locally\")\n",
        "            mr.Markdown(\"3. Use FTP/file browser to download from server\")\n",
        "\n",
        "        except Exception as e:\n",
        "            mr.Markdown(f\"### ‚ùå Error processing file\")\n",
        "            mr.Markdown(f\"**Error details:** {str(e)}\")\n",
        "            import traceback\n",
        "            mr.Markdown(f\"```\\n{traceback.format_exc()}\\n```\")\n",
        "else:\n",
        "    mr.Markdown(\"### üëã Welcome to Advanced NLP Text Classification Dashboard\")\n",
        "    mr.Markdown(\"**‚ú® ADVANCED FEATURES:**\")\n",
        "    mr.Markdown(\"- üåç **100+ languages** supported with intelligent detection\")\n",
        "    mr.Markdown(\"- üßπ **Enhanced text cleaning** with encoding normalization\")\n",
        "    mr.Markdown(\"- üîç **Intelligent meaningless text filtering** (N/A, nil, etc.)\")\n",
        "    mr.Markdown(\"- üîÑ **Confidence-based translation** with adjustable threshold\")\n",
        "    mr.Markdown(\"- üí≠ **Advanced sentiment analysis** with multiple methods\")\n",
        "    mr.Markdown(\"- üìä **Comprehensive statistics** and sample results\")\n",
        "    mr.Markdown(\"\")\n",
        "    mr.Markdown(\"### üìã How to Use:\")\n",
        "    mr.Markdown(\"1. Upload your CSV or Excel file containing 'Conversation Id' and 'transcripts' columns\")\n",
        "    mr.Markdown(\"2. Enable/disable automatic translation\")\n",
        "    mr.Markdown(\"3. Adjust translation confidence threshold (0.0-1.0)\")\n",
        "    mr.Markdown(\"   - Higher = more selective (only translate high-confidence detections)\")\n",
        "    mr.Markdown(\"   - Lower = more aggressive (translate more texts)\")\n",
        "    mr.Markdown(\"4. Click 'Run NLP Pipeline' to start processing\")\n",
        "    mr.Markdown(\"\")\n",
        "    mr.Markdown(\"### üì¶ Required Packages:\")\n",
        "    mr.Markdown(\"```bash\")\n",
        "    mr.Markdown(\"pip install textblob afinn langdetect deep-translator pandas openpyxl mercury\")\n",
        "    mr.Markdown(\"```\")\n",
        "    mr.Markdown(\"\")\n",
        "    mr.Markdown(\"### üì§ Output Columns:\")\n",
        "    mr.Markdown(\"- **Conversation Id**: Original identifier\")\n",
        "    mr.Markdown(\"- **Consumer_Text**: Original extracted text\")\n",
        "    mr.Markdown(\"- **Cleaned_Text**: Normalized and cleaned text\")\n",
        "    mr.Markdown(\"- **Translated_Text**: English translation (blank if English)\")\n",
        "    mr.Markdown(\"- **Category**: Main category classification\")\n",
        "    mr.Markdown(\"- **Subcategory**: Specific subcategory\")\n",
        "    mr.Markdown(\"- **Sentiment**: positive / negative / neutral / blank\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7RZbkflqV0T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyhoANT6qV0U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP Mercury Env",
      "language": "python",
      "name": "nlp_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}